{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements for this notebook:\n",
    "1. Internet connection (to download corpora and tokenizer data with calls to nltk.download())\n",
    "2. The following packages:\n",
    "  1. nltk (Anaconda or PIP command line install : pip install -U nltk OR conda install nltk)\n",
    "  2. gensim (pip install -U gensim)\n",
    "  3. scikit-learn v0.18.1 (pip install -U scikit-learn)\n",
    "  4. matplotlib (pip install -U matplotlib)\n",
    "  5. numpy (pip install -U numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives of this notebook are to illustrate how we can do the following with word embeddings:\n",
    "1. Train some embeddings from scratch\n",
    "2. Explore embeddings vectors\n",
    "3. Use these for an NLP task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "\n",
    "import nltk\n",
    "nltk.__version__\n",
    "\n",
    "import gensim\n",
    "gensim.__version__\n",
    "\n",
    "import numpy as np\n",
    "np.__version__\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import sklearn\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now that we're done with imports, let's download a few datasets\n",
    "## First some document sets (corpora)\n",
    "## Then some resources for wordlists and tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time nltk.download('brown')\n",
    "%time nltk.download('movie_reviews')\n",
    "%time nltk.download('treebank')\n",
    "\n",
    "# Let's download the PUNKT tokenizer first so that we can use tokenize words and sentences\n",
    "%time nltk.download('punkt')\n",
    "\n",
    "# Let's download stopwords so we can plot them later\n",
    "%time nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Now we can import these\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.corpus import treebank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's look at a few sentences from each of the 3 document sets\n",
    "* Brown -- First million word corpus created in 1961 at Brown University\n",
    "* Movie Reviews -- Reviews of movies with sentiment labels\n",
    "* Penn Treebank -- Widely used dataset for part-of-speech tagging and other NLP tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(brown.sents()[0])\n",
    "print(movie_reviews.sents()[0])\n",
    "print(treebank.sents()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pro Tip : These corpora are relatively small.  We may not get great results.  Certainly not as large as GoogleNews, Wikipedia, PubMed, etc.  Our mileage may vary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many sentences do we have in each corpus?\n",
    "print('Brown sentence count : ', len(brown.sents()))\n",
    "print('Movie Review sentence count : ', len(movie_reviews.sents()))\n",
    "print('Treebank sentence count : ', len(treebank.sents()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pro Tip : Before we go on, it's good to know about the GenSim documentation for models, training, querying, etc:\n",
    "https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Establish some parameters for training word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Word2vec has two flavors -- Skip-gram (SG) and continuous bag of words (CBOW)\n",
    "## Skipgram takes longer to train, so we'll disable it\n",
    "W2V_SKIP_GRAM = 0\n",
    "# Determine how many dimensions we want our word vectors to have in the end\n",
    "W2V_DIMENSIONS = 200\n",
    "# Minimum count of a token's occurrences in a corpus to be considered for training\n",
    "W2V_MIN_COUNT = 3\n",
    "\n",
    "# How many worker threads should we use to train?  Depends on your hardware...\n",
    "W2V_WORKERS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we can train some word2vec models and time them.\n",
    "## This doesn't take to long since the documents sets are relatively small and we can easily work with the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time brown_model = Word2Vec(brown.sents(), \\\n",
    "                             sg = W2V_SKIP_GRAM, \\\n",
    "                             size = W2V_DIMENSIONS, \\\n",
    "                             min_count = W2V_MIN_COUNT, \\\n",
    "                             workers = W2V_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time movie_model = Word2Vec(movie_reviews.sents(), \\\n",
    "                             sg = W2V_SKIP_GRAM, \\\n",
    "                             size = W2V_DIMENSIONS, \\\n",
    "                             min_count = W2V_MIN_COUNT, \\\n",
    "                             workers = W2V_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time treebank_model = Word2Vec(treebank.sents(), \\\n",
    "                                sg = W2V_SKIP_GRAM, \\\n",
    "                                size = W2V_DIMENSIONS, \\\n",
    "                                min_count = W2V_MIN_COUNT, \\\n",
    "                                workers = W2V_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that these are trained, we can inspect the final vocabulary sizes for each model\n",
    "print('Brown Model vocab size : ', len(brown_model.wv.vocab))\n",
    "print('Movie Review Model vocab size : ', len(movie_model.wv.vocab))\n",
    "print('Treebank Model vocab size : ', len(treebank_model.wv.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question : Given these vocabulary sizes, which one do we expect would be the worst in translating to other tasks like text classification, information extraction, etc?\n",
    "* Brown?\n",
    "* Movie Reviews?\n",
    "* Treebank?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's see which words are in-vocabulary and out-of-vocabulary for each set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do you think that the word \"Spielberg\" will be in each of these corpora?\n",
    "\n",
    "VOCABULARY_WORD_1 = 'spielberg'\n",
    "\n",
    "if VOCABULARY_WORD_1 in brown_model.wv.vocab:\n",
    "    print('Word found in Brown corpus')\n",
    "else:\n",
    "    print('Word NOT found in Brown corpus')\n",
    "    \n",
    "if VOCABULARY_WORD_1 in movie_model.wv.vocab:\n",
    "    print('Word found in Movie Review corpus')\n",
    "else:\n",
    "    print('Word NOT found in Movie Review corpus')\n",
    "    \n",
    "if VOCABULARY_WORD_1 in treebank_model.wv.vocab:\n",
    "    print('Word found in Treebank corpus')\n",
    "else:\n",
    "    print('Word NOT found in Treebank corpus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now choose another word of your own to see if it's in the vocabulary for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# do you think that the word \"Spielberg\" will be in each of these corpora?\n",
    "\n",
    "VOCABULARY_WORD_2 = 'CHOOSE_YOUR_WORD_HERE'\n",
    "\n",
    "if VOCABULARY_WORD_2 in brown_model.wv.vocab:\n",
    "    print('Word found in Brown corpus')\n",
    "else:\n",
    "    print('Word NOT found in Brown corpus')\n",
    "    \n",
    "if VOCABULARY_WORD_2 in movie_model.wv.vocab:\n",
    "    print('Word found in Movie Review corpus')\n",
    "else:\n",
    "    print('Word NOT found in Movie Review corpus')\n",
    "    \n",
    "if VOCABULARY_WORD_2 in treebank_model.wv.vocab:\n",
    "    print('Word found in Treebank corpus')\n",
    "else:\n",
    "    print('Word NOT found in Treebank corpus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save / Load models -- This is what allows us to train them and then transfer them to be portable for other tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we go any further, we can save one of our models\n",
    "brown_file_name = 'brown_' + time.strftime(\"%m_%d_%Y\")\n",
    "brown_model.save(brown_file_name)\n",
    "print('Saved Model to : ' + brown_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and here's how we can load a model back in...\n",
    "brown_loaded_model = Word2Vec.load(brown_file_name)\n",
    "print('Brown LOADED Model vocab size : ', len(brown_loaded_model.wv.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's start inspect our newly trained vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's inspect what some of these vectors look like\n",
    "print(brown_model.wv['business'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's ask the model which words have the most similar vectors to a few query words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(movie_model.wv.most_similar(positive = ['movie'], topn = 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tom Hanks is a film actor -- what other words behave in a similar way in the movie corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(movie_model.wv.most_similar(positive = ['hanks'], topn = 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What about a similar word like 'talk' in a generic corpus like the Brown corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(brown_model.wv.most_similar(positive = ['talk'], topn = 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now try a few of your own in each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(brown_model.wv.most_similar(positive = ['YOUR_OWN_WORD_HERE'], topn = 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(movie_model.wv.most_similar(positive = ['YOUR_OWN_WORD_HERE'], topn = 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note : This model is small with a small vocabulary so if a word doesn't exist, \n",
    "# it will throw an exception when trying to access it directly\n",
    "print(treebank_model.wv.most_similar(positive = ['YOUR_OWN_WORD_HERE'], topn = 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another thing we can do for each model is ask how similar two words are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's see how similar certain word pairs might be\n",
    "TERM_SIMILARITY_1 = 'movie'\n",
    "TERM_SIMILARITY_2 = 'film'\n",
    "\n",
    "print(brown_model.wv.similarity(TERM_SIMILARITY_1, TERM_SIMILARITY_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's try another pair\n",
    "TERM_SIMILARITY_3 = 'computer'\n",
    "TERM_SIMILARITY_4 = 'life'\n",
    "\n",
    "print(brown_model.wv.similarity(TERM_SIMILARITY_3, TERM_SIMILARITY_4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Vector Composition](vector_composition.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Now let's see if we can do some vector arithmetic to see if our model can perform well on analogy tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question : vec(families) + vec(city) - vec(family) = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this gives what we might expect from the BROWN corpus\n",
    "RELATIONSHIP_WORD_1 = 'families'\n",
    "RELATIONSHIP_WORD_2 = 'city'\n",
    "RELATIONSHIP_WORD_3 = 'family'\n",
    "\n",
    "#print(movie_model.wv.most_similar(positive=[RELATIONSHIP_WORD_1, RELATIONSHIP_WORD_2], negative=[RELATIONSHIP_WORD_3]))\n",
    "print(brown_model.wv.most_similar(positive=[RELATIONSHIP_WORD_1, RELATIONSHIP_WORD_2], negative=[RELATIONSHIP_WORD_3]))\n",
    "# apparently this corpus does not have one of our target words (KING)\n",
    "#treebank_model.most_similar(positive=[RELATIONSHIP_WORD_1, RELATIONSHIP_WORD_2], negative=['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question : vec(films) + vec(movie) - vec(film) = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this gives what we might expect from the MOVIE corpus\n",
    "RELATIONSHIP_WORD_1 = 'films'\n",
    "RELATIONSHIP_WORD_2 = 'movie'\n",
    "RELATIONSHIP_WORD_3 = 'film'\n",
    "\n",
    "print(movie_model.wv.most_similar(positive=[RELATIONSHIP_WORD_1, RELATIONSHIP_WORD_2], negative=[RELATIONSHIP_WORD_3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can we replicate the famous example from this paper:\n",
    "## If not, why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"King - Man + Woman ~~ Queen\"\n",
    "# http://www.aclweb.org/anthology/N13-1#page=784\n",
    "RELATIONSHIP_WORD_1 = 'woman'\n",
    "RELATIONSHIP_WORD_2 = 'king'\n",
    "RELATIONSHIP_WORD_3 = 'man'\n",
    "\n",
    "print(movie_model.wv.most_similar(positive=[RELATIONSHIP_WORD_1, RELATIONSHIP_WORD_2], negative=[RELATIONSHIP_WORD_3]))\n",
    "print(brown_model.wv.most_similar(positive=[RELATIONSHIP_WORD_1, RELATIONSHIP_WORD_2], negative=[RELATIONSHIP_WORD_3]))\n",
    "# apparently this corpus does not have one of our target words (KING)\n",
    "#treebank_model.most_similar(positive=[RELATIONSHIP_WORD_1, RELATIONSHIP_WORD_2], negative=['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's try to visualize some of the embeddings vectors with reduced dimensions by using a visualization method called t-distributed stochastic neighbor embedding (t-SNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embeddings(w2v_model, target_terms):\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    term_vectors = []\n",
    "    # let's make sure that a term we want is in the model\n",
    "    for target_term in target_terms:\n",
    "        if target_term in w2v_model.wv.vocab:\n",
    "            term_vectors.append(w2v_model.wv[target_term])\n",
    "    Y = tsne.fit_transform(term_vectors)\n",
    "    \n",
    "    # let's make this plot a decent size...\n",
    "    # Get current size\n",
    "    fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "    # Set figure width and height\n",
    "    fig_size[0] = 15\n",
    "    fig_size[1] = 9\n",
    "    plt.rcParams[\"figure.figsize\"] = fig_size\n",
    " \n",
    "    plt.scatter(Y[:, 0], Y[:, 1])\n",
    "    for label, x, y in zip(target_terms, Y[:, 0], Y[:, 1]):\n",
    "        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "    plt.show()\n",
    "    \n",
    "max_movie_sentences_to_visualize = 8\n",
    "visualization_terms = set()\n",
    "for sentence in movie_reviews.sents()[:max_movie_sentences_to_visualize]:\n",
    "    for token in sentence:\n",
    "        visualization_terms.add(token)\n",
    "    \n",
    "plot_embeddings(movie_model, list(visualization_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embeddings_3d(w2v_model, target_terms, min_word_length = 2):\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "    tsne = TSNE(n_components=3, random_state=0)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    term_vectors = []\n",
    "    filtered_terms = []\n",
    "    # let's make sure that a term we want is in the model\n",
    "    for target_term in target_terms:\n",
    "        # make sure that we're either not filtering or its above a certain length\n",
    "        if min_word_length < 0 or len(target_term) > min_word_length:\n",
    "            if target_term in w2v_model.wv.vocab:\n",
    "                term_vectors.append(w2v_model.wv[target_term])\n",
    "                filtered_terms.append(target_term)\n",
    "    Y = tsne.fit_transform(term_vectors)\n",
    "    \n",
    "    # let's make this plot a decent size...\n",
    "    # Get current size\n",
    "    fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "    # Set figure width and height\n",
    "    fig_size[0] = 15\n",
    "    fig_size[1] = 9\n",
    "    plt.rcParams[\"figure.figsize\"] = fig_size\n",
    " \n",
    "    fig = plt.gcf()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    ax.scatter(Y[:,0], Y[:,1], Y[:,2], marker='.')\n",
    "    for i, word in enumerate(filtered_terms):\n",
    "        ax.text(x=Y[i,0], y=Y[i,1], z=Y[i,2], s=word)\n",
    "    plt.show()\n",
    "    \n",
    "max_movie_sentences_to_visualize = 5\n",
    "visualization_terms = set()\n",
    "for sentence in movie_reviews.sents()[105:107]:\n",
    "    for token in sentence:\n",
    "        visualization_terms.add(token)\n",
    "    \n",
    "plot_embeddings_3d(movie_model, list(visualization_terms))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
