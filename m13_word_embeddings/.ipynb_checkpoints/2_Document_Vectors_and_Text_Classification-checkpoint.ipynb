{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements for this notebook:\n",
    "1. Internet connection (to download corpora and tokenizer data with calls to nltk.download())\n",
    "2. The following packages:\n",
    "  1. nltk (Anaconda or PIP command line install : pip install -U nltk OR conda install nltk)\n",
    "  2. gensim (pip install -U gensim)\n",
    "  3. scikit-learn v0.18.1 (pip install -U scikit-learn)\n",
    "  4. matplotlib (pip install -U matplotlib)\n",
    "  5. numpy (pip install -U numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives of this notebook are to illustrate how we can do the following with word embeddings:\n",
    "1. Introduce Document Vectors (Doc2Vec)\n",
    "2. Compare sentence vectors\n",
    "3. Use these for an NLP task -- Text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OK, so having a vector to represent a sequence of words (sentence, paragraph or document) would be useful, but how does it work?  \n",
    "## It's very similar to word2vec.  Word2vec predicts a word based on its neighbors or neighbors based on a word.  Doc2Vec is extremely similar but it adds the concept of a \"Document ID\" which can represent any sequence of text (sentence, paragraph or document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Brief Description of Doc2Vec](doc2vec.png)\n",
    "\n",
    "Le, Q., & Mikolov, T. (2014, January). Distributed representations of sentences and documents. In International Conference on Machine Learning (pp. 1188-1196)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import nltk\n",
    "nltk.__version__\n",
    "import gensim\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "gensim.__version__\n",
    "\n",
    "import sklearn\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "sklearn.__version__\n",
    "import numpy as np\n",
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download our corpora\n",
    "%time nltk.download('brown')\n",
    "%time nltk.download('movie_reviews')\n",
    "%time nltk.download('treebank')\n",
    "# Let's download the PUNKT tokenizer first so that we can use tokenize words and sentences\n",
    "%time nltk.download('punkt')\n",
    "# Let's download stopwords so we can plot them later\n",
    "%time nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.corpus import treebank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up parameters for Doc2Vec much like we did previously with Word2Vec..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of dimensions we want for our sentence/document vectors\n",
    "D2V_DIMENSIONS = 100\n",
    "# number of times a word much occur\n",
    "D2V_MIN_COUNT = 3\n",
    "# How many worker threads should we use to train?  Depends on your hardware...\n",
    "D2V_WORKERS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Don't worry about understanding the code and class below.  It's not usually this complicated to train Doc2Vec, but it helps us on an experiment at the end of the notebook\n",
    "## For now, understand that this allows us to \"wrap\" each sentence/document so that each one can be treated as distinct during training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PRO TIP : This is a generator class (notice the __iter__() and the yield)\n",
    "# This makes setting up a \"TaggedDocument\" class much easier and it becomes almost essential when training large \n",
    "# models for either Word2Vec or Doc2Vec since some corpora are so large that we cannot keep them in memory while training\n",
    "# the vector model.  Therefore, you can \"iterate\" through files or database rows and only keep batches of them \n",
    "# in memory at a time\n",
    "class TaggedSentenceGenerator(object):\n",
    "    def __init__(self, nltk_corpus, max_training_documents = -1):\n",
    "        self.nltk_corpus = nltk_corpus\n",
    "        self.max_training_documents = max_training_documents\n",
    "    def __iter__(self):\n",
    "        sent_idx = 0\n",
    "        sentences = self.nltk_corpus.sents()\n",
    "        \n",
    "        if self.max_training_documents > 0:\n",
    "            sentence_count_before = len(sentences)\n",
    "            sentences = sentences[:self.max_training_documents]\n",
    "            print('Using a smaller training set.  Reducing from size : {0} to {1}'.format(sentence_count_before, len(sentences)))\n",
    "            \n",
    "        for sent in sentences:\n",
    "            sent_idx += 1\n",
    "            # NOTE : These tags are not actually used during training time, but they are used\n",
    "            # to index and potentially query to find similar sentences/paragraphs/documents in GenSim\n",
    "            yield TaggedDocument(words=sent, tags=['SENT_%s' % sent_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we can train a Doc2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "print('Training a Doc2Vec model.  This can take some time...')\n",
    "movie_d2v_model = Doc2Vec(TaggedSentenceGenerator(movie_reviews), \n",
    "                                size = D2V_DIMENSIONS, \n",
    "                                min_count = D2V_MIN_COUNT, \n",
    "                                workers = D2V_WORKERS)\n",
    "\n",
    "print('Done training Doc2Vec model.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now that we have a model, let's try it out by comparing some sentences by their vectors using infer_vector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this example, the average embeddings vectors for sentence_1 and sentence_2 would be the same\n",
    "# However, doc2vec sees them as different.  This will allow context to be handled differently and a classifier\n",
    "# will be able to learn about these differences\n",
    "\n",
    "sentence_1 = 'I loved this great movie'\n",
    "sentence_2 = 'I loved this fantastic film'\n",
    "sentence_3 = 'boring and terrible'\n",
    "sentence_texts = [sentence_1, sentence_2, sentence_3]\n",
    "\n",
    "sentence_vectors = []\n",
    "for sentence_text in sentence_texts:\n",
    "    sentence_vec = movie_d2v_model.infer_vector(nltk.tokenize.word_tokenize(sentence_text))\n",
    "    sentence_vec = sentence_vec.reshape(1, -1)\n",
    "    sentence_vectors.append(sentence_vec)\n",
    "\n",
    "for i in range(len(sentence_vectors)):\n",
    "    for j in range(len(sentence_vectors)):\n",
    "        if i == j:\n",
    "            continue\n",
    "        print('*************')\n",
    "        print('{0} similarity score comparing [{1}] to [{2}]'.format(\n",
    "             sklearn.metrics.pairwise.cosine_similarity(sentence_vectors[i], sentence_vectors[j]),\n",
    "             sentence_texts[i],\n",
    "             sentence_texts[j]))\n",
    "\n",
    "print('DONE comparing sentences')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's try to visualize some sentence vectors with t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sentences(d2v_model, target_sentences):\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    sentence_vectors = []\n",
    "    # let's make sure that a term we want is in the model\n",
    "    for target_sentence in target_sentences:\n",
    "        document_tokens = target_sentence.split()\n",
    "        sentence_vector = d2v_model.infer_vector(document_tokens)\n",
    "        sentence_vectors.append(sentence_vector)\n",
    "    Y = tsne.fit_transform(sentence_vectors)\n",
    "    \n",
    "    # let's make this plot a decent size...\n",
    "    # Get current size\n",
    "    fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "    # Set figure width and height\n",
    "    fig_size[0] = 15\n",
    "    fig_size[1] = 9\n",
    "    plt.rcParams[\"figure.figsize\"] = fig_size\n",
    " \n",
    "    plt.scatter(Y[:, 0], Y[:, 1])\n",
    "    for label, x, y in zip(target_sentences, Y[:, 0], Y[:, 1]):\n",
    "        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "    plt.show()\n",
    "    \n",
    "target_plot_sentences = ['I absolutely adored the movie', 'I loved it more than any other movie', 'I hated it']\n",
    "plot_sentences(movie_d2v_model, target_plot_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's use these sentence vectors in a text classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [movie_reviews.words(fileid) for fileid in movie_reviews.fileids()]\n",
    "# these are the categories\n",
    "y_categories = [movie_reviews.categories(fileid)[0] for fileid in movie_reviews.fileids()]\n",
    "# this maps the strings to 1 -> positive 0 -> negative (anything else)\n",
    "y = [1 if category == 'pos' else 0 for category in y_categories ]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 777)\n",
    "\n",
    "print('TRAIN set size : {}'.format(len(X_train)))\n",
    "print('TEST set size : {}'.format(len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's transform all of our sentences into vectors to use them in text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "X_train_vectors = [movie_d2v_model.infer_vector(doc) for doc in X_train]\n",
    "X_test_vectors = [movie_d2v_model.infer_vector(doc) for doc in X_test]\n",
    "\n",
    "print('TRAIN transformed vector size : {}'.format(len(X_train_vectors)))\n",
    "print('TEST transformed vector size : {}'.format(len(X_test_vectors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a model using these transformed vectors and compute our F1 score (balanced metric of Precision and Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print('Training the model...  this could take some time')\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_vectors, y_train)\n",
    "\n",
    "d2v_f1 = f1_score(y_test, lr.predict(X_test_vectors))\n",
    "print('Doc2Vec F1 : {}'.format(d2v_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# As one more experiment, let's evaluate the impact of how many documents we use for Doc2Vec training\n",
    "## What happens if we use only 500 documents instead of thousands?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "SMALL_DOCUMENT_COUNT = 500\n",
    "\n",
    "print('Training a smaller Doc2Vec model.  This can take some time...')\n",
    "movie_d2v_model_small = Doc2Vec(TaggedSentenceGenerator(movie_reviews, max_training_documents = SMALL_DOCUMENT_COUNT), \n",
    "                                size = D2V_DIMENSIONS, \n",
    "                                min_count = D2V_MIN_COUNT, \n",
    "                                workers = D2V_WORKERS)\n",
    "\n",
    "print('Done training Doc2Vec model.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "X_train_vectors_small = [movie_d2v_model_small.infer_vector(doc) for doc in X_train]\n",
    "X_test_vectors_small = [movie_d2v_model_small.infer_vector(doc) for doc in X_test]\n",
    "\n",
    "print('TRAIN smaller transformed vector size : {}'.format(len(X_train_vectors)))\n",
    "print('TEST smaller transformed vector size : {}'.format(len(X_test_vectors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print('Training the smaller model...  this could take some time')\n",
    "lr_small = LogisticRegression()\n",
    "lr_small.fit(X_train_vectors_small, y_train)\n",
    "\n",
    "d2v_f1_small = f1_score(y_test, lr_small.predict(X_test_vectors_small))\n",
    "print('Doc2Vec (smaller training size) F1 : {}'.format(d2v_f1_small))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
