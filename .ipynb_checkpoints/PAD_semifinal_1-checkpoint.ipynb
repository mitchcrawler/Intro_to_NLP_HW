{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main NLP Class\n",
    "\n",
    "from pipeUtils import Document\n",
    "from pipeUtils import Annotation\n",
    "import re\n",
    "\n",
    "class PadClassificationSystem:\n",
    "    def __init__(self):\n",
    "        #initiate necessary components        \n",
    "        self.target_rules=self.getTargetRegexes()        \n",
    "        self.negation_rules = self.getNegRegexes()\n",
    "                \n",
    "    def process(self, document):\n",
    "        document_id = document.document_id\n",
    "        ann_index=0\n",
    "        for reg in self.target_rules:\n",
    "            for match in reg.finditer(document.text):\n",
    "                ann_id = 'NLP_'+ str(document_id) + '_' + str(ann_index)\n",
    "                ann_index=ann_index+1\n",
    "                new_annotation = Annotation(start_index=int(match.start()), \n",
    "                                    end_index=int(match.end()), \n",
    "                                    type='pad_annotation',\n",
    "                                    ann_id = ann_id\n",
    "                                    )\n",
    "                new_annotation.spanned_text = document.text[new_annotation.start_index:new_annotation.end_index]\n",
    "\n",
    "                # Check negation right before the found target up to 30 charachers before, \n",
    "                # making sure that the pre-text does not cross the text boundary and is valid\n",
    "\n",
    "                if new_annotation.start_index - 30 > 0:\n",
    "                    pre_text_start = new_annotation.start_index - 30\n",
    "                else:\n",
    "                    pre_text_start = 0\n",
    "\n",
    "                # ending index of the pre_text is the beginning of the found target    \n",
    "                pre_text_end = new_annotation.start_index    \n",
    "\n",
    "                # substring the document text to identify the pre_text string\n",
    "                pre_text = doc.text[pre_text_start: pre_text_end]\n",
    "\n",
    "                # We do not need to know the exact location of the negation keyword, so re.search is acceptable\n",
    "                for neg_regex in self.negation_rules:\n",
    "                    if re.search(neg_regex, pre_text):\n",
    "                        new_annotation.attributes[\"Negation\"] =\"Negated\"\n",
    "\n",
    "                document.annotations.append(new_annotation)\n",
    "        \n",
    "        return document \n",
    "    \n",
    "    def getTargetRegexes(self):\n",
    "        target_regexes = []\n",
    "        regexes = [\n",
    "            r'\\bp(eri\\w+)?\\s*(a(rt\\w+)?|v(as\\w+)?)\\s*d(ise\\w+)?\\b',\n",
    "            r'\\ba(nk\\w+)?(\\s*\\-|\\\\|\\/)?\\s*b(ra\\w+)?\\s*i(nd\\w+)?\\b(\\W*)?((?:\\w+\\W+){1,5}?((0*(\\.\\d{1,2}))|(1*(\\.[4-9])))?)?',\n",
    "            \n",
    "        ]\n",
    "        for reg in regexes:\n",
    "            target_regexes.append(re.compile(reg, re.IGNORECASE))\n",
    "        return target_regexes\n",
    "\n",
    "    def getNegRegexes(self):\n",
    "        target_regexes = []\n",
    "        regexes = [\n",
    "            r'/\\bno\\b',\n",
    "            r'/no\\s*evidence\\s*of'  ,\n",
    "            r'/does\\s*not\\s*have',\n",
    "            r'/denies'\n",
    "        ]\n",
    "        for reg in regexes:\n",
    "            target_regexes.append(re.compile(reg, re.IGNORECASE))\n",
    "        return target_regexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc1\r\n",
      "-------\r\n",
      "\n",
      "Patient has peripheral artery disease. ---------- \n",
      "Patient also has PVD or peripheral vascular\n",
      "disease or pvd . \n",
      "\n",
      " The patient does not have any peripheral artery disease \n",
      "but has peripheral arterial disease . The patient denies having peripheral vascular disease . \n",
      " \n",
      "The patient has a femoral and illiac occlusion which is suggestive of peripheral arterial disease.\n",
      "\r\n",
      "-------\r\n",
      "NLP_Doc1_0 pad_annotation 13 38 peripheral artery disease \r\n",
      "NLP_Doc1_1 pad_annotation 69 72 PVD \r\n",
      "NLP_Doc1_2 pad_annotation 76 103 peripheral vascular\n",
      "disease \r\n",
      "NLP_Doc1_3 pad_annotation 107 110 pvd \r\n",
      "NLP_Doc1_4 pad_annotation 146 171 peripheral artery disease \r\n",
      "NLP_Doc1_5 pad_annotation 181 208 peripheral arterial disease \r\n",
      "NLP_Doc1_6 pad_annotation 237 264 peripheral vascular disease \r\n",
      "NLP_Doc1_7 pad_annotation 340 367 peripheral arterial disease \r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  test case\n",
    "nlp_system = PadClassificationSystem()\n",
    "doc_text = '''\n",
    "Patient has peripheral artery disease. ---------- \\nPatient also has PVD or peripheral vascular\\ndisease or pvd . \n",
    "\\n The patient does not have any peripheral artery disease \n",
    "but has peripheral arterial disease . The patient denies having peripheral vascular disease . \\n \n",
    "The patient has a femoral and illiac occlusion which is suggestive of peripheral arterial disease.\n",
    "'''\n",
    "doc=Document(text=doc_text, document_id='Doc1')\n",
    " \n",
    "out_doc=nlp_system.process(doc)\n",
    "print(out_doc.toString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeUtils import Annotation\n",
    "from pipeUtils import Document\n",
    " \n",
    "import os\n",
    "import glob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all test documents\n",
    "unid=\"u1166466\"\n",
    "project_1 = \"PAD_TRAIN\"\n",
    "project_2 = \"PAD_ABI\"\n",
    "path_1 = \"/home/\"+str(unid)+\"/BRAT/\"+str(unid)+\"/\"+project_1\n",
    "path_2 = \"/home/\"+str(unid)+\"/BRAT/\"+str(unid)+\"/\"+project_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'18600_2.txt': <pipeUtils.Document at 0x7f3a1c50fc18>,\n",
       " '21223_3.txt': <pipeUtils.Document at 0x7f3a1c49e470>,\n",
       " '25879_4.txt': <pipeUtils.Document at 0x7f3a1c4aba58>,\n",
       " '25879_5.txt': <pipeUtils.Document at 0x7f3a208af198>,\n",
       " '37_10.txt': <pipeUtils.Document at 0x7f3a1c50f240>,\n",
       " '37_11.txt': <pipeUtils.Document at 0x7f3a20cdf828>,\n",
       " '37_12.txt': <pipeUtils.Document at 0x7f3a1c50f4e0>,\n",
       " '37_13.txt': <pipeUtils.Document at 0x7f3a1c49eb70>,\n",
       " '37_14.txt': <pipeUtils.Document at 0x7f3a1c4b3048>,\n",
       " '37_15.txt': <pipeUtils.Document at 0x7f3a1c49e780>,\n",
       " '37_16.txt': <pipeUtils.Document at 0x7f3a1c4a5d30>,\n",
       " '37_17.txt': <pipeUtils.Document at 0x7f3a20ccff98>,\n",
       " '37_18.txt': <pipeUtils.Document at 0x7f3a1c504780>,\n",
       " '37_19.txt': <pipeUtils.Document at 0x7f3a1c504ac8>,\n",
       " '56_0.txt': <pipeUtils.Document at 0x7f3a1c50fba8>,\n",
       " '56_1.txt': <pipeUtils.Document at 0x7f3a2c12e5f8>,\n",
       " '56_2.txt': <pipeUtils.Document at 0x7f3a1c4b31d0>,\n",
       " '56_3.txt': <pipeUtils.Document at 0x7f3a1c504668>,\n",
       " '56_4.txt': <pipeUtils.Document at 0x7f39ac247d68>,\n",
       " '56_5.txt': <pipeUtils.Document at 0x7f3a20cdf7b8>,\n",
       " '56_6.txt': <pipeUtils.Document at 0x7f3a2c12e128>,\n",
       " '56_7.txt': <pipeUtils.Document at 0x7f3a2c12e160>,\n",
       " '56_8.txt': <pipeUtils.Document at 0x7f3a1c50ff28>,\n",
       " '56_9.txt': <pipeUtils.Document at 0x7f3a20cdf550>,\n",
       " '6677_0.txt': <pipeUtils.Document at 0x7f3a1c50f160>,\n",
       " '6677_1.txt': <pipeUtils.Document at 0x7f3a1c504978>}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_docs=dict()\n",
    "test_doc_paths = glob.glob(str(path_1+'/*.txt')) \n",
    "for d in test_doc_paths:\n",
    "    doc = Document()\n",
    "    #print(d)\n",
    "    doc.load_document_from_file(d)\n",
    "    #print(str(d[:-3])+'ann')\n",
    "    doc.load_annotations_from_brat(str(d[:-3])+'ann')\n",
    "    #print(os.path.basename(d))\n",
    "    test_docs[os.path.basename(d)]=doc\n",
    "\n",
    "\n",
    "test_docs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "def randomize_files(test_docs):\n",
    "    shuffle(test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "\n",
    "def get_training_and_testing_sets(test_docs):\n",
    "    split = 0.8\n",
    "    split_index = floor(len(test_docs) * split)\n",
    "    training = test_docs[:test_docs]\n",
    "    testing = test_docs[test_docs:]\n",
    "    return training, testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing the all notes\n",
    "nlp_system = PadClassificationSystem()\n",
    "\n",
    "for doc_id in  test_docs.keys():\n",
    "    nlp_system.process(test_docs.get(doc_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP = 0 FP = 13 FN = 0\n",
      "NLP_6677_1.txt_0 pad_annotation 207 210 PVD \n",
      "NLP_6677_1.txt_1 pad_annotation 3156 3159 PVD \n",
      "NLP_6677_1.txt_2 pad_annotation 2609 2642 ankle brachial index of\n",
      "     0.57 \n",
      "NLP_6677_0.txt_0 pad_annotation 207 210 PVD \n",
      "NLP_6677_0.txt_1 pad_annotation 3156 3159 PVD \n",
      "NLP_6677_0.txt_2 pad_annotation 2609 2642 ankle brachial index of\n",
      "     0.57 \n",
      "NLP_18600_2.txt_0 pad_annotation 419 422 pvd \n",
      "NLP_18600_2.txt_1 pad_annotation 1082 1114 ankle brachial\n",
      "     index could  \n",
      "NLP_18600_2.txt_2 pad_annotation 1400 1427 ankle brachial index could  \n",
      "NLP_25879_4.txt_0 pad_annotation 1419 1447 ankle brachial index is 0.71 \n",
      "NLP_25879_5.txt_0 pad_annotation 1419 1447 ankle brachial index is 0.71 \n",
      "NLP_21223_3.txt_0 pad_annotation 1665 1693 ankle-brachial indices were  \n",
      "NLP_21223_3.txt_1 pad_annotation 1755 1784 ankle-brachial index was 0.36 \n"
     ]
    }
   ],
   "source": [
    "tp_total = 0\n",
    "fp_total = 0\n",
    "fn_total = 0\n",
    "tp_list_total = []\n",
    "fp_list_total= []\n",
    "fn_list_total = []\n",
    "for doc_id in test_docs.keys():\n",
    "    tp, fp, fn, tp_list, fp_list, fn_list = (test_docs.get(doc_id)).compare_types_by_span('PAD','pad_annotation', False)\n",
    "    tp_total = tp_total + tp\n",
    "    fp_total = fp_total + fp\n",
    "    fn_total = fn_total + fn\n",
    "    tp_list_total.extend(tp_list)\n",
    "    fp_list_total.extend(fp_list)\n",
    "    fn_list_total.extend(fn_list)\n",
    "\n",
    "print('TP =',tp_total, 'FP =',fp_total, 'FN =',fn_total)\n",
    "\n",
    "if tp_total > 0 :\n",
    "    precision = tp_total / (tp_total + fp_total)\n",
    "    print('Precision=',round(precision,3))\n",
    "\n",
    "if tp_total > 0 :\n",
    "    recall = tp_total / (tp_total + fn_total)\n",
    "    print('Recall=',round(recall,3))\n",
    "\n",
    "for a in tp_list_total:\n",
    "    print(a[0].toString(),'||', a[1].toString())\n",
    "for a in fp_list_total:\n",
    "    print(a.toString())\n",
    "for a in fn_list_total:\n",
    "    print(a.toString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_total = 0\n",
    "fp_total = 0\n",
    "fn_total = 0\n",
    "tp_list_total = []\n",
    "fp_list_total= []\n",
    "fn_list_total = []\n",
    "attributes_to_compare=[]\n",
    "# To compare attributes, create a list of tuples for each pair to compare:\n",
    "# attributes_to_compare.append[(A1_type, A1_att_name, A1_att_value),(A2_type, A2_att_name, A2_att_value)]\n",
    "attributes_to_compare.append([('PAD', 'Negation', 'Negated'),('pad_annotation', 'Negation', 'Negated')])\n",
    "\n",
    "for doc_id in test_docs.keys():\n",
    "    tp, fp, fn, tp_list, fp_list, fn_list = (test_docs.get(doc_id)).\\\n",
    "    compare_types_by_span_and_attributes('PAD','pad_annotation', attributes_to_compare , False)\n",
    "    tp_total = tp_total + tp\n",
    "    fp_total = fp_total + fp\n",
    "    fn_total = fn_total + fn\n",
    "    tp_list_total.extend(tp_list)\n",
    "    fp_list_total.extend(fp_list)\n",
    "    fn_list_total.extend(fn_list)\n",
    "\n",
    "print('TP =',tp_total, 'FP =',fp_total, 'FN =',fn_total)\n",
    "\n",
    "if tp_total > 0 :\n",
    "    precision = tp_total / (tp_total + fp_total)\n",
    "    print('Precision=',round(precision,3))\n",
    "\n",
    "if tp_total > 0 :\n",
    "    recall = tp_total / (tp_total + fn_total)\n",
    "    print('Recall=',round(recall,3))\n",
    "\n",
    "#for a in tp_list_total:\n",
    "#    print(a[0].toString(),'||', a[1].toString())\n",
    "for a in fp_list_total:\n",
    "    print(a.toString())\n",
    "for a in fn_list_total:\n",
    "    print(a.toString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "import pandas as pd\n",
    "import getpass\n",
    "import re\n",
    "\n",
    "conn = pymysql.connect(host=\"mysql\",\n",
    "                       port=3306,user=\"jovyan\",\n",
    "                       passwd=getpass.getpass(\"Enter MySQL passwd for jovyan\"),db='mimic2')\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_data = pd.read_sql(\"\"\"SELECT subject_id, \n",
    "                      category, \n",
    "                      text FROM noteevents limit 100000 \"\"\",conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_nlp_system = PadClassificationSystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "counter = 0\n",
    "for index , row in pad_data.sample(10000).iterrows():    \n",
    "    doc = Document(document_id=str(row.subject_id) + '_' + str(index), text=row.text)\n",
    "    final_nlp_system.process(doc)\n",
    "    if(len(doc.annotations) > 0):\n",
    "        i = 1\n",
    "        for a in doc.annotations:\n",
    "            if( a.type == 'pad_annotation'):\n",
    "                neg_flag = 0\n",
    "                # Switch the flag to 1 when the mention is negated\n",
    "                if('definite_negated_existence' in a.attributes):\n",
    "                    neg_flag=1\n",
    "                ### Each row in the dictionary\n",
    "                record_id  = str(row.subject_id) + '_' + str(index)+'_'+str(i)\n",
    "                subject_id =  row.subject_id\n",
    "                note_id = str(row.subject_id) + '_' + str(index)\n",
    "                annotation_type = a.type\n",
    "                snippet = doc.text[int(a.start_index): int(a.end_index)]\n",
    "                out_list = [record_id, subject_id, note_id, annotation_type, \\\n",
    "                            a.start_index, a.end_index, \\\n",
    "                            snippet, neg_flag]\n",
    "                output.append(out_list)\n",
    "                i=i+1\n",
    "                counter=counter+1\n",
    "                # Print . after 10 identified records\n",
    "                if counter%10 == 0:\n",
    "                    print('.', end='')\n",
    "        else:\n",
    "            continue\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=['record_id','subject_id', 'note_id', 'annotation_type', 'span_start', 'span_end', 'PAD_snippet', 'neg_flag']\n",
    "result_data_frame = (pd.DataFrame(output, columns=columns))\n",
    "\n",
    "result_data_frame.describe()\n",
    "result_data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data_frame.to_csv('out_table.csv', index=False)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
