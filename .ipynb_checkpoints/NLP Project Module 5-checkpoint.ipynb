{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyRuSH.RuSH import RuSH\n",
    "from pyConTextNLP import pyConTextGraph\n",
    "from DocumentClassifier import DocumentClassifier \n",
    "from pyConTextNLP.utils import get_document_markups\n",
    "\n",
    "from DocumentClassifier import FeatureInferencer\n",
    "from DocumentClassifier import DocumentInferencer\n",
    "from nlp_pneumonia_utils import markup_sentence\n",
    "from itemData import get_item_data\n",
    "from visual import convertMarkups2DF\n",
    "\n",
    "\n",
    "# begin to define MyPipe class\n",
    "class MyPipe:\n",
    "    def __init__(self, sentence_rules, target_rules, context_rules, feature_inference_rule, document_inference_rule):\n",
    "        # initiate necessary components here        \n",
    "        self.sentence_segmenter = RuSH(sentence_rules)\n",
    "        self.targets=get_item_data(target_rules)\n",
    "        self.modifiers=get_item_data(context_rules)\n",
    "        self.feature_inferencer=FeatureInferencer(feature_inference_rule)\n",
    "        self.document_inferencer = DocumentInferencer(document_inference_rule)\n",
    "    \n",
    "    def process(self, doc_text):        \n",
    "        #process your input doc_text, return the required results\n",
    "        sentences=self.sentence_segmenter.segToSentenceSpans(doc_text)\n",
    "        #sentences=doc_text.split('\\n')\n",
    "        context_doc = pyConTextGraph.ConTextDocument()\n",
    "\n",
    "        for sentence in sentences:\n",
    "            sentence_text=doc_text[sentence.begin:sentence.end].lower()\n",
    "            # Process every sentence by adding markup\n",
    "            m = markup_sentence(sentence_text, modifiers=self.modifiers, targets=self.targets)\n",
    "            context_doc.addMarkup(m)\n",
    "            #context_doc.getSectionMarkups()\n",
    "        \n",
    "        markups = get_document_markups(context_doc)\n",
    "        annotations, relations, doc_txt = convertMarkups2DF(markups) \n",
    "        \n",
    "        inferenced_types = self.feature_inferencer.process(annotations, relations)\n",
    "        doc_class = self.document_inferencer.process(inferenced_types)\n",
    "        \n",
    "        return doc_class, context_doc, annotations, relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure your rules \n",
    "\n",
    "sentence_rules='KB/rush_rules.tsv'\n",
    "# you can point target_rules to a file path instead, if there are many rules\n",
    "target_rules='''\n",
    "Comments: ''\n",
    "Direction: ''\n",
    "Lex: peripheral arterial/vascular disease\n",
    "Regex: 'p(eri\\w+)?\\s*(a(rt\\w+)?|v(as\\w+)?)\\s*d(ise\\w+)?'\n",
    "Type: PAD\n",
    "\n",
    "'''\n",
    "# context rules are often lengthy, you can point context_rules to an external rule files instead\n",
    "context_rules='''\n",
    "Comments: ''\n",
    "Direction: forward\n",
    "Lex: 'history'\n",
    "Regex: ''\n",
    "Type: DEFINITE_AFFIRMED_EXISTENCE\n",
    "---\n",
    "Comments: ''\n",
    "Direction: forward\n",
    "Lex: 'h/o'\n",
    "Regex: ''\n",
    "Type: DEFINITE_AFFIRMED_EXISTENCE\n",
    "---\n",
    "Comments: ''\n",
    "Direction: forward\n",
    "Lex: ankle brachial index\n",
    "Regex: 'a(nk\\w+)?(\\s*\\-|\\\\|\\/)?\\s*b(ra\\w+)?\\s*i(nd\\w+)?(\\W*)?(?:\\w+\\W+){1,5}?(0*(\\.\\d{1,2}))|(1*(\\.[4-9]))'\n",
    "Type: DEFINITE_AFFIRMED_EXISTENCE\n",
    "---\n",
    "Comments: ''\n",
    "Direction: forward\n",
    "Lex: 'past'\n",
    "Regex: ''\n",
    "Type: DEFINITE_AFFIRMED_EXISTENCE\n",
    "---\n",
    "Comments: ''\n",
    "Direction: forward\n",
    "Lex: 'diagnosis'\n",
    "Regex: ''\n",
    "Type: DEFINITE_AFFIRMED_EXISTENCE\n",
    "---\n",
    "Comments: ''\n",
    "Direction: forward\n",
    "Lex: 'condition'\n",
    "Regex: ''\n",
    "Type: DEFINITE_AFFIRMED_EXISTENCE\n",
    "'''\n",
    "# define the feature inference rule\n",
    "feature_inference_rule='''\n",
    "#Conclusion type, Evidence type, Modifier values associated with the evidence\n",
    "AFFIRMED_CONCEPT,PAD,DEFINITE_AFFIRMED_EXISTENCE\n",
    "'''\n",
    "# define the document inference rule\n",
    "document_inference_rule='''\n",
    "#Conclusion Type at document level, Evidence type at mention level\n",
    "PAD_DOC,PAD\n",
    "\n",
    "#Default document type\n",
    "NO_PAD\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initiate an instance of MyPipe\n",
    "myPipe=MyPipe(sentence_rules, target_rules, context_rules, feature_inference_rule, document_inference_rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter MySQL passwd for jovyan········\n"
     ]
    }
   ],
   "source": [
    "import pymysql\n",
    "import pandas as pd\n",
    "import getpass\n",
    "\n",
    "conn = pymysql.connect(host=\"mysql\",\n",
    "                       port=3306,user=\"jovyan\",\n",
    "                       passwd=getpass.getpass(\"Enter MySQL passwd for jovyan\"),db='mimic2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_data = pd.read_sql(\"SELECT text FROM noteevents where text like '%peripheral%vascular disease%' limit 5\",conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write you code here, fill the results by processing each document through MyPipe\n",
    "\n",
    "results = {index: (myPipe.process(row.text.replace('\\n',' ')))for index, row in pad_data.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visual import view_pycontext_output\n",
    "from visual import view_pycontext_outputs\n",
    "from visual import Vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the context documents that you created in Exercise 2 and put them into a dictionary.\n",
    "context_docs=dict()\n",
    "i=0\n",
    "for item, row in results.items(): \n",
    "    doc_name = \"doc\" + str(i)\n",
    "    context_docs[doc_name]=row[1]\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "\t\t\t      <iframe src = \"tmp/test.html\" frameborder=\"0\" width = \"850\" height = \"225\">\n",
       "\t\t\t         Sorry your browser does not support inline frames.\n",
       "\t\t\t      </iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "view_pycontext_output(context_docs['doc1'], Vis(file_name=\"test.html\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "600c275f24a149a09076f2aff1c10a4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>interactive</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='i', max=4), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "view_pycontext_outputs(context_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyConTextNLP.utils import get_document_markups\n",
    "from visual import convertMarkups2DF\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore everything inside here, we will explain later\n",
    "from DocumentClassifier import DocumentClassifier\n",
    "from visual import Vis, view_pycontext_output\n",
    "pos_doc_type='FAM_BREAST_CA_DOC'\n",
    "TARGETS_FILE_PATH = 'KB/fam_bc_targets.yml'\n",
    "MODIFIERS_FILE_PATH = 'KB/fam_bc_modifiers.yml'\n",
    "FEATURE_INFERENCER_FILE_PATH = 'KB/fam_bc_featurer_inferences.csv'\n",
    "DOC_INFERENCER_FILE_PATH = 'KB/fam_bc_doc_inferences.csv'\n",
    "\n",
    "# clear just in case files/regular expressions have been updated\n",
    "vis = Vis(context_file=MODIFIERS_FILE_PATH, file_name=\"context_graph.html\")\n",
    "classifier = DocumentClassifier(TARGETS_FILE_PATH, MODIFIERS_FILE_PATH,\n",
    "                            FEATURE_INFERENCER_FILE_PATH, DOC_INFERENCER_FILE_PATH,\n",
    "                            {pos_doc_type})\n",
    "classifier.reset_saved_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'getDocument'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-119-abd6c3bb6e57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmarkups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_document_markups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_last_context_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_txt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvertMarkups2DF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarkups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pyConTextNLP/utils.py\u001b[0m in \u001b[0;36mget_document_markups\u001b[0;34m(document)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_document_markups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m\"\"\" Given a ConTextDocument return an ordered list of the ConTextmarkup objects consistituting the document\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     tmp = [(e[1],e[2]['sentenceNumber']) for e in document.getDocument().edges(data=True) if\n\u001b[0m\u001b[1;32m      6\u001b[0m            e[2].get('category') == 'markup']\n\u001b[1;32m      7\u001b[0m     \u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'getDocument'"
     ]
    }
   ],
   "source": [
    "markups=get_document_markups(classifier.get_last_context_doc())\n",
    "annotations, relations, doc_txt=convertMarkups2DF(markups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'annotations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-aa88721c2bba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mconcepts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# read all the target concepts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'vis_category'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'Target'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mconcepts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'markup_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'start'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'end'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'txt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'annotations' is not defined"
     ]
    }
   ],
   "source": [
    "concepts=dict()\n",
    "# read all the target concepts\n",
    "for index, row in annotations.iterrows():\n",
    "    if row['vis_category']=='Target':\n",
    "        concepts[row['markup_id']]=[row['type'], int(row['start']) ,int(row['end']),row['txt']]\n",
    "\n",
    "# attach related modifiers\n",
    "for index, row in relations.iterrows():\n",
    "    if row['arg2_cate']=='Target':\n",
    "        concepts[row['arg2_id']].append(row['type']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-115-eb899422f922>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'output.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcsvfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mspamwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsvfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mspamwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriterows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcepts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'csv' is not defined"
     ]
    }
   ],
   "source": [
    "with open('output.csv', 'w', newline='') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile)\n",
    "    spamwriter.writerows(list(concepts.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
