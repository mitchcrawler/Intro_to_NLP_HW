{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1:\n",
    "In class we have gone through a rule-based NLP pipeline by executing components one by one. Typically, we create a pipeline as a single class that links all modules.\n",
    "\n",
    "In this exercise, you will need to write a pipeline class. Let's call it **MyPipe**, it can be initiated through taking a set of rules. After that, it can be called through a **process** function, which can take a document text, and output a set of values: \n",
    "- document level classification\n",
    "- context document, which is an object of type pyConTextGraph.ConTextDocument;\n",
    "- annotations (in dataframe format);\n",
    "- relations (in dataframe format)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state your import here\n",
    "import re\n",
    "from PyRuSH.RuSH import RuSH\n",
    "from pyConTextNLP import pyConTextGraph\n",
    "from pyConTextNLP.utils import get_document_markups\n",
    "\n",
    "\n",
    "from DocumentClassifier import FeatureInferencer\n",
    "from DocumentClassifier import DocumentInferencer\n",
    "from nlp_pneumonia_utils import markup_sentence\n",
    "from itemData import get_item_data\n",
    "from visual import convertMarkups2DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'DocumentClassifier'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a0a6ab1f4515>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mDocumentClassifier\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFeatureInferencer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mDocumentClassifier\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDocumentInferencer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnlp_pneumonia_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmarkup_sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'DocumentClassifier'"
     ]
    }
   ],
   "source": [
    "# begin to define MyPipe class\n",
    "class MyPipe:\n",
    "    def __init__(self, sentence_rules, target_rules, context_rules, feature_inference_rule, document_inference_rule):\n",
    "        # initiate necessary components here\n",
    "        self.sentence_rules = sentence_rules\n",
    "        self.target_rules = target_rules\n",
    "        self.context_rules = context_rules\n",
    "        self.feature_inference_rule = feature_inference_rule\n",
    "        self.document_inference_rule = document_inference_rule\n",
    "    \n",
    "    def process(self, doc_text):        \n",
    "        #process your input doc_text, return the required results\n",
    "         # because there are too many sentence segmentation rules, let's read them from an external file\n",
    "        sentence_rules='KB/rush_rules.tsv'\n",
    "        # you can point target_rules to a file path instead, if there are many rules\n",
    "        target_rules='''\n",
    "        Comments: ''\n",
    "        Direction: ''\n",
    "        Lex: fever\n",
    "        Regex: ''\n",
    "        Type: FEVER\n",
    "        ---\n",
    "        Comments: ''\n",
    "        Direction: ''\n",
    "        Lex: high temperature\n",
    "        Regex: '1\\d\\d\\.\\d F'\n",
    "        Type: FEVER'''\n",
    "        # context rules are often lengthy, you can point context_rules to an external rule files instead\n",
    "        context_rules='''Comments: ''\n",
    "        Direction: forward\n",
    "        Lex: 'no'\n",
    "        Regex: ''\n",
    "        Type: DEFINITE_NEGATED_EXISTENCE\n",
    "        ---\n",
    "        Comments: ''\n",
    "        Direction: forward\n",
    "        Lex: 'denies'\n",
    "        Regex: ''\n",
    "        Type: DEFINITE_NEGATED_EXISTENCE\n",
    "        '''\n",
    "        # define the feature inference rule\n",
    "        feature_inference_rule='''\n",
    "        #Conclusion type, Evidence type, Modifier values associated with the evidence\n",
    "        NEGATED_CONCEPT,FEVER,DEFINITE_NEGATED_EXISTENCE\n",
    "        '''\n",
    "        # define the document inference rule\n",
    "        document_inference_rule='''\n",
    "        #Conclusion Type at document level, Evidence type at mention level\n",
    "        FEVER_DOC,FEVER\n",
    "        \n",
    "        #Default document type\n",
    "        NO_FEVER\n",
    "        '''\n",
    "        return doc_class, context_doc, annotations, relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your pipeline class is defined, you can use it multiple times for different set of rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2:\n",
    "Now you can select documents from the MIMIC database (limit to 5 documents that contain your target concept), write a script to process all of them, and output a dictionary which uses document name as keys and document level classification as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure your rules \n",
    "sentence_rules=''\n",
    "target_rules=''\n",
    "context_rules=''\n",
    "feature_inference_rule=''\n",
    "document_inference_rule=''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initiate an instance of MyPipe\n",
    "myPipe=MyPipe(sentence_rules, target_rules, context_rules, feature_inference_rule, document_inference_rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write you code here, fill the results by processing each document through MyPipe\n",
    "results=dict()  # this dictionary will contain document names as keys and a document-level classification as values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3:\n",
    "\n",
    "Now you get the results, but how can you be sure if they are correct? Let's dig a little deeper to visualize them. \n",
    "\n",
    "Hint: **view_pycontext_output** can take in a list of ConTextDocuments and visualize them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visual import view_pycontext_output\n",
    "from visual import Vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the context documents that you created in Exercise 2.\n",
    "context_docs=[]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the file name to contain your UNID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_pycontext_output(context_docs, Vis(file_name=\"YOUR_UNID.html\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Please make a screenshot of the visualization file and submit it as homework assignment.***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
