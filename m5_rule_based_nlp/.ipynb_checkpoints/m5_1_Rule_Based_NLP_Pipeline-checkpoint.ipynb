{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule Based NLP Pipeline\n",
    "\n",
    "This notebook explains the idea of building rule based systems in plain language. You will know what it is and how it works after finishing this module. Let's start with a simple task with basic intuition.\n",
    "\n",
    "Suppose we have a sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "input='The patient came in with fever.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we want to know if this sentence tells us about whether the patient has fever. What is your basic instinct to implement a solution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A naïve solution\n",
    "\n",
    "A very naïve solution could be to just find the string \"the patient came in with fever.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "rule='The patient came in with fever'\n",
    "print(rule in input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this solution has a big problem. **What is it**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Think about other cases. There may be a case like: \n",
    "- 'The pt got a fever.'   \n",
    "- 'The pt developed a fever.' \n",
    "- 'Patient presented with fever.' \n",
    "- 'Findings: fever. 'etc. \n",
    "\n",
    "What can you do to handle cases like these? \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A better and neat solution\n",
    "\n",
    "As you may have noticed, there is a common characteristics in all the examples above. They all have the word \"fever.\" So probably we do not need to match all the words in a sentence to determine if patient has fever. Instead, we can simply check if a sentence has the keyword \"fever.\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "rule='fever'\n",
    "print(rule in input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "\n",
    "In fact, we just implemented a very simple named entity recognition solution! Of course it's not perfect, but it works. Named entity recognition is a subtask of NLP, which aims to identify the named entities or concepts in free text.\n",
    "\n",
    "In the example above,\"fever\" is our target concept. And we use simple string match to identify it from a sentence. Besides string match, another commonly used technique for named entity recognition is regular expression. \n",
    "\n",
    "Here is an exercise to try:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input='He got a high temperature yesterday, T 102.5.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can you identify the numeric temperature? Of course for the sentence of alone, you can still use string match. What if another case has a different number? In these cases, using regular expressions will be more efficient.\n",
    "\n",
    "If you don't know what regular expression is, you can check out [this notebook](m5_2_Regular_Expression.ipynb) to get a taste of RegEx.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "## define your regular expression rule to match \"T 102.5\"\n",
    "rule=r'' #Add Regular expression to capture temperature\n",
    "pattern=re.compile(rule)\n",
    "res=pattern.search(input)\n",
    "\n",
    "## test your result, see if it matches your expectation\n",
    "res.group()=='T 102.5'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the output above shows \"*False*\", please check the code above. Variable \"rule\" has to be defined to match the target pattern.\n",
    "\n",
    "----------\n",
    "\n",
    "More advanced named entity recognition solutions can deal with syntactical similarities to match similar phrases or words, even in slightly different order; or handle semantic similarities to match synonyms. The topic of named entity recognition is more advanced and we are not going to cover that here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "Usually, just to identify the named entities is not enough. **Why?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "What if it says \n",
    "- 'Discussed about possible fever.' or \n",
    "- 'No fever.'\n",
    "\n",
    "How do we identify that a patient has or does not have fever based on these sentences?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Detection\n",
    "\n",
    "From the examples above, we now see that how important the context information is, especially for a clinical NLP, where many concepts are expressed as \"*not present in patient*.\" Now let's think about how to implement a context detector. Again, let's start from a very intuitive way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple solution\n",
    "Use regular expressions to match the context information and the target concepts together. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases =['The patient denies any fever.', 'The pt denies fever.', 'The patient denies any chills or fever.' , 'Patient presented with fever.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule=r'denies( \\w+)* fever'\n",
    "negation_pattern=re.compile(rule)\n",
    "for case in cases:\n",
    "    res=negation_pattern.search(case)\n",
    "    print('negated' if res is not None else 'not negated' , ' -- ', case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course this solution works. But, what if we have a long list of vocabularies for the target concepts? It seems we have to write every negation word combined with every synonym of the target concept. That is neither efficient nor convenient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A smarter solution\n",
    "\n",
    "If we separate the context identification (negated vs not negated) from the target concept identification, we can then check whether a target concept is modified by any context information. This way we do not have to define all possible combinations, but simply loop through the list of context rules for each concept. And most importantly, once we build this set of context rules, we can reuse it in other similar tasks without major tweaks.\n",
    "\n",
    "This is the basic outline of the ConText algorithm (Harkema H, Dowling JN, Thornblade T, Chapman WW. [ConText: an algorithm for determining negation, experiencer, and temporal status from clinical reports.](https://www.ncbi.nlm.nih.gov/pubmed/19435614) J Biomed Inform 2009;42(5):839-851). \n",
    "\n",
    "Though the ConText algorithm is much more complicated, using it is easy. For more detailed information about how to work with the context algorithm, please check [m5_3_Intro_pyConText.ipynb](m5_3_Intro_pyConText.ipynb)  notebook.\n",
    "\n",
    "Here we are going to implement a very simple version of context algorithm:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to define the rule to identify the target concept---fever\n",
    "target_rule='fever'\n",
    "\n",
    "# Second, we need to define the rules to identify the negation clue -- denies\n",
    "context_rule='denies'\n",
    "\n",
    "for input in cases: \n",
    "    print(input,' -- ',target_rule,\"is\", 'negated' if target_rule in input and context_rule in input else 'not negated')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context detection within a document\n",
    "\n",
    "Now we've learned how to detect the context within a sentence. In real practice, we need to do the same thing to muliptle sentences within a document, which brings another issue. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input='''\n",
    "No vomiting, chest pain, shortness of breath, nausea, dizziness, or chills on arrival.\n",
    "On operative day three, the patient fever was detected with temperature 101.5 F.\n",
    "After 3 days no fever was detected.\n",
    "Patient came back for a follow up, denies fever.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a human, we can certainly understand that the word \"No\" does not apply to the \"fever\", because they are in different sentences. But how can we implement a solution to make a computer thinks in a similiar way?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Segmentation\n",
    "\n",
    "This is why we need sentence segmentation. Sentence segmentation/detection is another subtask of NLP. It sets up the boundaries for the downstream NLP components to work on. Let's implement a single sentence segmenter to demonstrate how it helps for context detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This segmenter a split sentence based on the new line character.\n",
    "sentences=input.split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply our previous NER and context detection for each sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the rule to identify the target concept---fever\n",
    "target_rule=re.compile(r'fever')\n",
    "\n",
    "# Define the rules to identify the negation clue---No, no, denies\n",
    "context_rule= re.compile(r'[Nn]o|denies')\n",
    "\n",
    "# Let's define a \"check\" function to mimic the context detection\n",
    "def check(sentence):\n",
    "    if target_rule.search(sentence):\n",
    "        if context_rule.search(sentence):\n",
    "            return \"negated\"\n",
    "        else:\n",
    "            return \"not negated\"\n",
    "    else:\n",
    "        return \"NA\"\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(check(sentence), ' -- ' , sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And again, there are many ways to segment sentences. You can check [5_4_Sentence_Segmentation.ipynb](5_4_Sentence_Segmentation.ipynb) notebook to learn more about it.\n",
    "\n",
    "Now it seems we already have all the pieces that we need to build to the NLP pipeline.\n",
    "\n",
    "....No exactly. Remember our goal is to get a conclusion at document level to say whether a document has mentioned something that we care about. So, currently we only get conclusions at the sentence level, there is still one piece missing, where we need to make the document conclusion based on the sentence level conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Classification\n",
    "\n",
    "The rule-based the document classification is completely based on the rules that define inferences from the sentence level conclusions towards the document level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=input.split('\\n')\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it a simple, let's just use string match to find the target concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the rules in a list\n",
    "target_rule=re.compile(r'fever|101.5 F')\n",
    "\n",
    "# Define the rules to identify the negation clue---no\n",
    "context_rule= re.compile(r'[Nn]o|denies')\n",
    "\n",
    "def check(sentence):\n",
    "    if target_rule.search(sentence):\n",
    "        if context_rule.search(sentence):\n",
    "            return \"negated\"\n",
    "        else:\n",
    "            return \"not negated\"\n",
    "    else:\n",
    "        return \"NA\"\n",
    "\n",
    "results=[]\n",
    "for sentence in sentences:\n",
    "    results.append(check(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a list of sentence-level conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define a simple function to make a conclusion  by taking this list as the input, and output three type of document level conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If at least one result is not negated, the document is labeled as not negated.\n",
    "def classifier(results):\n",
    "    for result in results:\n",
    "        if result == \"not negated\":\n",
    "            return \"not negated\"      \n",
    "    return \"NA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your classifier\n",
    "print(classifier(results)==\"not negated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP pipeline\n",
    "Now you may have some ideas about the concept: **pipeline**. We need several different NLP components, and oragnize them together in an appropriate order to process the input information.\n",
    "\n",
    "To sum up the key points we learned from above, a rule based NLP pipeline includes the following components:\n",
    "\n",
    "- sentence segmenter, \n",
    "- named entity recognizer, \n",
    "- context detector, and \n",
    "- a document classifier.\n",
    "\n",
    "Now, **forget what you have coded above, those are just used to help you understand** the functionality of each component and why we need it. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a state-of-the-art rule-based NLP pipeline\n",
    "\n",
    "To build a state-of-art rule-based NLP pipeline, we definitely don't need to start from scratch. Instead, we will just borrow the components from others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyRuSH.RuSH import RuSH\n",
    "from pyConTextNLP import pyConTextGraph\n",
    "from pyConTextNLP.utils import get_document_markups\n",
    "\n",
    "\n",
    "from DocumentClassifier import FeatureInferencer\n",
    "from DocumentClassifier import DocumentInferencer\n",
    "from nlp_pneumonia_utils import markup_sentence\n",
    "from itemData import get_item_data\n",
    "from visual import convertMarkups2DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because there are too many sentence segmentation rules, let's read them from an external file\n",
    "sentence_rules='KB/rush_rules.tsv'\n",
    "# you can point target_rules to a file path instead, if there are many rules\n",
    "target_rules='''\n",
    "Comments: ''\n",
    "Direction: ''\n",
    "Lex: fever\n",
    "Regex: ''\n",
    "Type: FEVER\n",
    "---\n",
    "Comments: ''\n",
    "Direction: ''\n",
    "Lex: high temperature\n",
    "Regex: '1\\d\\d\\.\\d F'\n",
    "Type: FEVER'''\n",
    "# context rules are often lengthy, you can point context_rules to an external rule files instead\n",
    "context_rules='''Comments: ''\n",
    "Direction: forward\n",
    "Lex: 'no'\n",
    "Regex: ''\n",
    "Type: DEFINITE_NEGATED_EXISTENCE\n",
    "---\n",
    "Comments: ''\n",
    "Direction: forward\n",
    "Lex: 'denies'\n",
    "Regex: ''\n",
    "Type: DEFINITE_NEGATED_EXISTENCE\n",
    "'''\n",
    "# define the feature inference rule\n",
    "feature_inference_rule='''\n",
    "#Conclusion type, Evidence type, Modifier values associated with the evidence\n",
    "NEGATED_CONCEPT,FEVER,DEFINITE_NEGATED_EXISTENCE\n",
    "'''\n",
    "# define the document inference rule\n",
    "document_inference_rule='''\n",
    "#Conclusion Type at document level, Evidence type at mention level\n",
    "FEVER_DOC,FEVER\n",
    "\n",
    "#Default document type\n",
    "NO_FEVER\n",
    "'''\n",
    "\n",
    "sentence_segmenter = RuSH(sentence_rules)\n",
    "feature_inferencer=FeatureInferencer(feature_inference_rule)\n",
    "document_inferencer = DocumentInferencer(document_inference_rule)\n",
    "\n",
    "targets=get_item_data(target_rules)\n",
    "modifiers=get_item_data(context_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Let's split sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=sentence_segmenter.segToSentenceSpans(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See what the document was splitted into\n",
    "for sentence in sentences:\n",
    "    print(\"Sentence({}-{}):\\t{}\".format(sentence.begin, sentence.end, input[sentence.begin:sentence.end]))\n",
    "    print('\\n'+'-'* 100+'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Apply Context algorithm to each sentence\n",
    "\n",
    "The pyConText has already built in a NER and ConText detector, which can be applied in a single call. So we don't need to use them separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate a pyConTextGraph to hold the pyConText output\n",
    "context_doc = pyConTextGraph.ConTextDocument()\n",
    "\n",
    "for sentence in sentences:\n",
    "    sentence_text=input[sentence.begin:sentence.end].lower()\n",
    "    # Process every sentence by adding markup\n",
    "    m = markup_sentence(sentence_text, modifiers=modifiers, targets=targets)\n",
    "    context_doc.addMarkup(m)\n",
    "    context_doc.getSectionMarkups()\n",
    "    print(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the context output in XML format\n",
    "print(context_doc.getXML())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Apply document classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graphy output is not convenient to either use or display, let's convert it into [pandas](https://pandas.pydata.org/) [dataframes](https://pandas.pydata.org/pandas-docs/stable/dsintro.html#dataframe) first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert graphic markups into dataframe    \n",
    "markups = get_document_markups(context_doc)\n",
    "annotations, relations, doc_txt = convertMarkups2DF(markups) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's easier to read the results in tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(annotations)\n",
    "display(relations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can use the anntoations and relations as input to make the document inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply inferences for document classication\n",
    "inferenced_types = feature_inferencer.process(annotations, relations)\n",
    "print('After inferred from modifier values, we got these types:\\n '+str(inferenced_types))\n",
    "doc_class = document_inferencer.process(inferenced_types)\n",
    "print('\\nDocument classification: '+ doc_class )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visual import view_pycontext_output\n",
    "from visual import Vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_pycontext_output(context_doc, Vis(file_name=\"context_graph.html\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image above illustrates relationships built by the pyConText algorithm.\n",
    "\n",
    "### Have quiestions? Please ask!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
